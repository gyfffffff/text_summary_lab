lr: 0.01
batch_size: 28
epoches: 100
patience: 5
device: cuda
optim: adam
# weight_decay: 1e-4
lr_decay: 0
clip: True
max_norm: 10

model: "lstm2lstm"
hidden_size: 128
embed_size: 64
num_layers: 1
vocab_size: 1303 # 不加1会有索引溢出

log_dir: log
res_dir: res

data_dir: data
